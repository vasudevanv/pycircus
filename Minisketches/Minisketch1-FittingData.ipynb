{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting parameters with SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a logistic function will be fitted to synthetic data using scipy's ``optimize`` function.\n",
    "\n",
    "The logistic function is defined as\n",
    "$$ f(x) = \\frac{a}{1+\\exp\\big(-(b x - x_0)\\big)} ,$$\n",
    "\n",
    "where $a$ is the maximum value of the $f(x)$, $b$ determines the steepness of the curve, and $x_0$ is the midpoint of the curve, which is also the point of half-maximum and the inflection point.\n",
    "\n",
    "First, we'll code the logistic function in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(params,x):\n",
    "    \"\"\"Logistic function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list or numpy array\n",
    "      the three parameters of the logistic function\n",
    "      \n",
    "    x : numpy array\n",
    "      the explanatory variable\n",
    "   \n",
    "    Return\n",
    "    ------\n",
    "    numpy array\n",
    "      the output of the logistic function\n",
    "\n",
    "    \"\"\"\n",
    "    return params[0]/(1+np.exp(-x*params[1] - params[2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $a=b=x_{0}=1$, the logistic function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-8,8,100)          # creating a set of x values\n",
    "y = logistic([1,1,1],x)            # parsing the x values through the logistic function\n",
    "\n",
    "# Plotting\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('x', size=16)\n",
    "plt.ylabel('y', size=16)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the y lies between 0 and 1. Owing to the way we've coded it, the logistic function cannot be negative.\n",
    "\n",
    "Now, let's create some synthetic data to fit to. We'll set $a=3$, $b=2$, and $x_0=0$ and add some noise to the data. The noise will be sampled from a Gaussian distribution with a mean of 0 and standard deviation of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-8,8,50)          \n",
    "y = logistic([3,2,0],x)  + np.random.normal(0,0.1,size=50)\n",
    "\n",
    "# Having a look at what that looks like:\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x', size=16)\n",
    "plt.ylabel('y', size=16)\n",
    "plt.title('Synthetic data')\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fit, we'll create a function that returns the residuals of our trial fit to the noisy data. When fitting the parameters, we'll be minimising the residuals, which in this case is the sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residuals(params):\n",
    "    predicted = logistic(params,x)\n",
    "    return np.sum((y-predicted)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from an initial guess of the parameters, will minimise the residuals with SciPy's `optimize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_guess = [1,1,1]\n",
    "fit = optimize.minimize(residuals,initial_guess,method='BFGS')\n",
    "print \"The predicted parameters are\", fit.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look very close to the true parameters of the model:  $a=3$, $b=2$, and $x_0=0$. \n",
    "\n",
    "We can illustrate this nicely with yet another plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = logistic(fit.x,x)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x,predicted,color=\"red\")\n",
    "plt.xlabel('x', size=16)\n",
    "plt.ylabel('y', size=16)\n",
    "plt.title('Synthetic data with fitted parameters')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Curve fitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will be fitting an exponential curve to noisy data using scipy's ``curve_fit`` function. \n",
    "\n",
    "$$ f(x) = a * \\mathrm{exp} \\left( -bx \\right) + c$$\n",
    "\n",
    "The curve_fit function uses the Levenberg-Marquardt algorithm to perform non-linear least-square optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy.random\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def exponential(x, a, b, c):\n",
    "    \"\"\"Exponential function\n",
    "    \n",
    "    f(x) = a*exp(-b*x) + c\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: numpy.ndarray\n",
    "        Numpy array representing range of data\n",
    "        \n",
    "    a, b, c: float\n",
    "        Parameters of exponential function\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    numpy array\n",
    "      the output of the exponential function\n",
    "    \"\"\"\n",
    "    return a*numpy.exp(-b*x) + c\n",
    "\n",
    "\n",
    "# Create noisy dataset\n",
    "x = numpy.linspace(0,4,50)\n",
    "a = 2.5\n",
    "b = 1.5\n",
    "c = 1.0\n",
    "temp = exponential(x, a, b, c) \n",
    "temp = temp + 0.1 * numpy.random.normal(size=len(temp)) \n",
    "\n",
    "### Curve fitting\n",
    "fit_params, fit_covariances = curve_fit(exponential, x, temp)\n",
    "a_fit, b_fit, c_fit = tuple(fit_params)\n",
    "\n",
    "print(\"Estimated parameters a: {0:<4.2f}, b: {1:<4.2f}, c: {2:<4.2f}\".format(a_fit, b_fit, c_fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax.set_ylabel('Temperature', fontsize = 16)\n",
    "ax.set_xlabel('time (s)', fontsize = 16)\n",
    "ax.set_xlim(0,4.1)\n",
    "\n",
    "# Plot the data \n",
    "ax.errorbar(x, temp, fmt = 'ro', yerr = 0.1)\n",
    "\n",
    "# Best fit curve \n",
    "ax.plot(x, exponential(x, a_fit, b_fit, c_fit),'k-', lw=2)\n",
    "\n",
    "# Plot the +/- sigma curves\n",
    "# The square root of the diagonal covariance matrix  \n",
    "# element is the uncertainty on the fit parameter.\n",
    "sigma_a, sigma_b, sigma_c = numpy.sqrt(fit_covariances[0,0]), \\\n",
    "np.sqrt(fit_covariances[1,1]), \\\n",
    "np.sqrt(fit_covariances[2,2])\n",
    "ax.plot(x, exponential(x, a_fit + sigma_a, b_fit - sigma_b, c_fit + sigma_c), 'b-')\n",
    "ax.plot(x, exponential(x, a_fit - sigma_a, b_fit + sigma_b, c_fit - sigma_c), 'b-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regression using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some data\n",
    "data_x = np.linspace(1.0, 10.0, 100)[:, np.newaxis]\n",
    "data_y = np.sin(data_x) + 0.1 * np.power(data_x, 2) + 0.5 * np.random.randn(100, 1)\n",
    "\n",
    "# Normalization\n",
    "model_order = 6\n",
    "data_x = np.power(data_x, range(model_order))\n",
    "data_x /= np.max(data_x, axis=0)\n",
    "\n",
    "# Generate test and training data set using a shuffling procedure\n",
    "order = np.random.permutation(len(data_x))\n",
    "portion = 20\n",
    "test_x = data_x[order[:portion]]\n",
    "test_y = data_y[order[:portion]]\n",
    "train_x = data_x[order[portion:]]\n",
    "train_y = data_y[order[portion:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient function\n",
    "def get_gradient(w, x, y):\n",
    "    y_estimate = x.dot(w).flatten()\n",
    "    error = (y.flatten() - y_estimate)\n",
    "    mse = (1.0/len(x))*np.sum(np.power(error, 2))\n",
    "    gradient = -(1.0/len(x)) * error.dot(x)\n",
    "    return gradient, mse\n",
    "\n",
    "\n",
    "\n",
    "w = np.random.randn(model_order)\n",
    "alpha = 0.5\n",
    "tolerance = 1e-5\n",
    "\n",
    "# Perform Stochastic Gradient Descent\n",
    "epochs = 1\n",
    "decay = 0.99\n",
    "batch_size = 10\n",
    "iterations = 0\n",
    "while True:\n",
    "    order = np.random.permutation(len(train_x))\n",
    "    train_x = train_x[order]\n",
    "    train_y = train_y[order]\n",
    "    b=0\n",
    "    while b < len(train_x):\n",
    "        tx = train_x[b : b+batch_size]\n",
    "        ty = train_y[b : b+batch_size]\n",
    "        gradient = get_gradient(w, tx, ty)[0]\n",
    "        error = get_gradient(w, train_x, train_y)[1]\n",
    "        w -= alpha * gradient\n",
    "        iterations += 1\n",
    "        b += batch_size\n",
    "    \n",
    "    # Keep track of our performance\n",
    "    if epochs%100==0:\n",
    "        new_error = get_gradient(w, train_x, train_y)[1]\n",
    "        print (\"Epoch: %d - Error: %.4f\" %(epochs, new_error))\n",
    "    \n",
    "        # Stopping Condition\n",
    "        if abs(new_error - error) < tolerance:\n",
    "            print (\"Converged.\")\n",
    "            break\n",
    "        \n",
    "    alpha = alpha * (decay ** int(epochs/1000))\n",
    "    epochs += 1\n",
    "\n",
    "print (\"w =\",w)\n",
    "print (\"Test Cost =\", get_gradient(w, test_x, test_y)[1])\n",
    "print (\"Total iterations =\", iterations)\n",
    "\n",
    "# Plot result\n",
    "y_model = np.polyval(w[::-1], np.linspace(0,1,100))\n",
    "plt.plot(np.linspace(0,1,100), y_model, c='g', label='Model')\n",
    "plt.scatter(train_x[:,1], train_y, c='b', label='Train Set')\n",
    "plt.scatter(test_x[:,1], test_y, c='r', label='Test Set')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
